---
title: "Crazy Trolley"
date: 2016-09-02T16:19:04+01:00
draft: false
tags: ["AI", "French"]
description: "This is a short description of the page"
---
Ce premier mois à Lyon fut l’occasion de retourner à l’écriture avec un petit article pour l’Insatiable, le journal étudiant de l’INSA de Lyon.

Dans cet article (Crazy Trolley en page 7), je reviens sur l’arrivée prochaine des voitures autonomes, des décisions morales liées à leur utilisation et des personnes derrières ces même décisions.

Ce numéro de septembre est disponible à cette adresse. https://drive.google.com/file/d/0B4DsqT58kYbzUVE0U3ZnaWc0Nzg/view

Si vous avez des remarques ou des questions, n’hésitez pas à me contacter. =)

Pour ceux qui ne peuvent télécharger l’article, voici sa retranscription.

***
# Crazy Trolley

L’intelligence artificielle est là ! Elle nous défait aux échecs, au jeu de Go et même à Starcraft ! Quelles seront ses limites ? Pouvons-nous laisser les décisions morales à la machine ?

Prenons une situation sensiblement simple. Et complètement improbable oui d’accord, mais c’est un exemple bon sang !

Un tramway fou dévale une rue à toute allure, aucun moyen de le stopper, sa vitesse est trop grande et il arrive à une intersection. À cette intersection, deux chemins : le premier, avec cinq personnes sur les rails, est celui vers lequel le véhicule se dirige. Le second, avec seulement une personne sur la voie, sera emprunté par le tramway si vous décidez de tirer un levier. Le choix est à vous. Ne rien faire et laisser 5 personnes mourir ou alors agir et tuer une personne.  Alors, vous tirez le levier ?

Ce problème moral dit “du tramway” et souvent utilisé pour des études d’éthique et de neurosciences. Mais maintenant, modifions légèrement les variables. Ce n’est plus vous qui décidez de tirer le levier, mais une intelligence artificielle.
Hasta la vista, Baby !

Utiliser ce dilemme sur des intelligences artificielles n’est pas anodin. En effet, en devant prendre des choix pouvant impacter la vie ou l’intégrité d’un être humain, les systèmes intelligents se doivent aujourd’hui de suivre certaines règles. Ces règles, même si souvent légèrement différentes, empruntent la voie des lois de la robotique de l’auteur de science-fiction Isaac Asimov.

Ces lois, au nombre de trois, sont les suivantes :
1. un robot ne peut porter atteinte à un être humain, ni, en restant passif, permettre qu’un être humain soit exposé au danger ;
2. un robot doit obéir aux ordres qui lui sont donnés par un être humain, sauf si de tels ordres entrent en conflit avec la première loi ;
3. un robot doit protéger son existence tant que cette protection n’entre pas en conflit avec la première ou la deuxième loi.

Ce système de pensée possède beaucoup de lacunes, mais nous l’utiliserons comme base pour notre analyse. Si vous voulez en savoir plus sur ces lois, vous pouvez vous procurer les livres du cycle des robots, par Isaac Asimov.

Nous voyons donc se distinguer deux options : dans la première, l’IA laisse mourir 5 personnes par son inaction, et dans la seconde, son action tue une personne. Dans les deux cas, le système ne respecte pas les lois. Cependant, il est nécessaire que celui-ci fasse un choix.

Plusieurs solutions peuvent sembler correctes et morales dans une situation, mais elles dépendront toujours de notre subjectivité. Pour représenter ce souci de subjectivité et essayer d’y répondre, le MIT a mis en place une application pour récolter des choix humains dans des cas de décisions de voitures autonomes.

Nous vous invitons donc à jeter un œil à “Moral Machine”, l’outil du MIT. Le jeu vous proposera de répondre à treize situations impliquant des dégâts humains. Vous pourrez ensuite comparer vos résultats de “sauvetage” entre joueurs, suivant les différentes situations et de configuration d’âge, de genre, de respect des lois lors du choix, etc. Si vous souhaitez vous prêter au jeu, vous trouverez ce projet sur http://moralmachine.mit.edu
C’est pas moi, c’est l’IA !

Si les décisions prises par l’IA ne peuvent pas respecter les lois dans tous les cas, il convient pourtant de définir un comportement pour ces situations extrêmes.

Des biais doivent donc être incorporés pour permettre aux systèmes de les surmonter. Le système va alors prendre une décision basée sur des données déjà fournies et, pour un programme plus avancé, sur des données qu’il aura lui-même pu récupérer et qui vont donc influencer ses décisions.

L’IA devient-elle donc responsable de ses actes ? Une célèbre phrase de Dijkstra (mathématicien et informaticien du XXe siècle) allait ainsi “Se demander si un ordinateur peut penser est aussi intéressant que de se demander si un sous-marin peut nager”. Suivant cette maxime, il devient donc aisé de réaliser que les IA, aussi développées qu’elles puissent être, seront pour encore longtemps de simples algorithmes et non des machines douées de conscience.

Nous avons vu qu’une IA prendra ses décisions en suivant des comportements et des biais, tacites ou non. La dernière question est donc de se demander qui décidera de ses variables comportementales et surtout, qui en prendra la responsabilité ? En effet, il n’est pas envisageable, aujourd’hui, d’incriminer un cerveau électronique pour ses choix. Tout problème juridique se répercutera donc sur les programmeurs et décideurs à l’origine de la machine.
Assurance bêtise naturelle ?

C’est dans cette optique que se profile un nouvel acteur, l’assureur ! En effet, qu’il s’agisse d’une voiture, d’un système de sécurité ou d’une habitation intelligente, ces ensembles sont souvent couverts par différentes polices d’assurance. Les contrats devront donc préciser certaines décisions de l’IA vous concernant. Votre voiture pourra-t-elle vous mener à la mort si cette action sauve 5 personnes ? Si vous en doutez, alors souscrivez à notre nouvelle assurance auto garantie sans homicide !

On se rend donc compte de la complexité des choix posés par les systèmes experts. Et ce ne sont pas vraiment les choix des IA même qui vont poser problème, mais bel et bien ceux des structures impliquant ces machines. Il advient donc de se poser deux grandes questions.

Premièrement, comment permettre au plus grand nombre de comprendre l’impact de tels algorithmes sur leur vie ? Nous vivons dans un monde où la connaissance ou non des processus informatiques a généré de grandes disparités sociales. L’arrivée de nouvelles avancées technologiques risque d’accentuer encore l’importance de ce fossé.

Enfin, qui sera le garant des décisions finales de nos robots et IA ? Pouvons-nous nous déresponsabiliser de cette obligation en la transmettant à des structures comme les assurances ? Et dans ce cas, quelles seront les conséquences de ce pouvoir donné à des tiers ?

Amélia
